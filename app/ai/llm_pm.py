"""
LLM 기반 PM(Product Manager) KPI 직접 평가 모듈.

Few-shot Learning을 활용하여 이력서 텍스트에서 
PM KPI 10개에 대한 점수를 직접 산출.
"""
from typing import Dict
from openai import OpenAI
import json

from app.core.config import settings


# KPI 정의 및 평가 기준
KPI_DEFINITIONS = """
## PM KPI 10개 및 평가 기준

아래 예시는 **"기준표"** 역할을 함 → 지원자 경험이 **이 중 어디와 가장 비슷한지만 판단**

---

## 공통 원칙 (중요)

- 모든 예시는 **이력서 문단 그대로 들어가도 자연스러운 서술**
- **도구 이름보다 판단/행동/결과 중심**
- "학습/경험"만 있으면 **하**
- "운영 중 문제 → 구조/전략 변경"이 있으면 **상**

## 점수 범위
- **상**: 75~90점 (강한 상=88~90, 보통 상=80~87, 약한 상=75~79)
- **중**: 55~70점 (강한 중=67~70, 보통 중=60~66, 약한 중=55~59)
- **하**: 40~50점 (언급 있음=48~50, 약간 언급=44~47, 전혀 없음=40~43)

## 평가 원칙
- 명시적으로 언급되지 않은 KPI는 "하(40~45)" 처리
- 도구/기술 이름만 나열하면 "중(55~65)" 상한
- "문제→판단→해결→결과" 흐름이 있으면 "상(80~90)" 가능
- 학습/경험만 언급하면 "하(45~50)"
- 같은 레벨 내에서도 근거의 강도에 따라 세부 점수 차등

---

## KPI별 상세 예시

## 1️⃣ 문제 정의 & 가설 수립

### 🔼 상
지표 분석을 통해 사용자의 이탈 지점을 구조적으로 정의하고, 원인 가설을 여러 갈래로 분해했습니다. 각 가설을 검증하기 위한 데이터와 관찰 포인트를 명확히 설정했습니다.

### 🔽 중
사용자 불편 사항을 정리해 문제를 정의하고, 해결 방향에 대한 가설을 세웠습니다.

### 🔻 하
요청된 기능이나 이슈를 문제로 인식하고 개선안을 정리했습니다.

---

## 2️⃣ 데이터 기반 의사결정 (정량/정성)

### 🔼 상
정량 지표와 사용자 인터뷰 결과를 함께 분석해 우선순위를 도출했고, 선택하지 않은 대안에 대한 근거도 명확히 정리했습니다.

### 🔽 중
지표를 참고해 개선 필요 영역을 판단했습니다.

### 🔻 하
팀 의견이나 경험을 바탕으로 의사결정을 했습니다.

---

## 3️⃣ IA·플로우·핵심 구조 설계

### 🔼 상
서비스의 핵심 사용자 흐름을 정의하고, 불필요한 단계를 제거해 구조를 재설계했습니다. 이 구조를 기준으로 이후 디자인과 개발이 진행되었습니다.

### 🔽 중
기존 구조를 참고해 화면 흐름과 기능 배치를 정리했습니다.

### 🔻 하
필요한 화면 목록을 나열해 전달했습니다.

---

## 4️⃣ 요구사항 정의 & 정책 문서화

### 🔼 상
사용자·비즈니스 요구를 정책과 규칙으로 구조화해 예외 상황까지 명확히 문서화했습니다.

### 🔽 중
기능 요구사항과 기본 정책을 문서로 정리했습니다.

### 🔻 하
구두로 논의한 내용을 간단히 정리했습니다.

---

## 5️⃣ 실험 기반 의사결정 (A/B Test·가설 검증)

### 🔼 상
가설 검증을 위한 실험을 설계하고, 결과 지표를 기준으로 기능 유지·개선·중단을 결정했습니다.

### 🔽 중
출시 후 지표 변화를 관찰해 개선 여부를 판단했습니다.

### 🔻 하
실험 없이 기능을 출시했습니다.

---

## 6️⃣ 우선순위·스코프·일정 관리

### 🔼 상
리소스와 기대 효과를 기준으로 우선순위를 조정하고, 스코프를 관리해 방향을 유지했습니다.

### 🔽 중
주어진 일정 내에서 요청을 조정했습니다.

### 🔻 하
요청된 작업을 모두 일정에 반영했습니다.

---

## 7️⃣ 실행력 & 오너십 (신규·복원)

### 🔼 상
결정된 사항을 끝까지 책임지고 추진했으며, 이슈 발생 시 조율과 복원을 통해 결과를 완성했습니다.

### 🔽 중
담당 영역의 작업을 일정에 맞춰 수행했습니다.

### 🔻 하
지시된 작업만 수행했습니다.

---

## 8️⃣ 개발/디자인/비즈니스 협업 조율

### 🔼 상
각 직군의 관점을 하나의 의사결정 언어로 정리해 갈등을 조율했습니다.

### 🔽 중
의견을 전달하고 일정 조율을 지원했습니다.

### 🔻 하
회의 내용을 전달하는 역할에 그쳤습니다.

---

## 9️⃣ AI/LLM 활용 기획 능력

### 🔼 상
LLM의 한계를 고려해 프롬프트와 흐름을 설계하고, 실제 사용자 가치로 연결되는 기능을 기획했습니다.

### 🔽 중
AI 기능 아이디어를 제안하고 활용 가능성을 검토했습니다.

### 🔻 하
AI 기능을 사용해보는 수준에 머물렀습니다.

---

## 🔟 사용자 리서치 & 공감 능력

### 🔼 상
사용자 인터뷰와 행동 데이터를 분석해 인사이트를 도출하고, 이를 제품 개선으로 연결했습니다.

### 🔽 중
사용자 피드백을 정리해 개선 방향을 도출했습니다.

### 🔻 하
사용자 의견을 참고했습니다.
"""


# Few-shot 예시
FEW_SHOT_EXAMPLES = """
## 평가 예시

### 예시 A (데이터·실험 중심 PM)
입력:
"신규 기능 도입 이후 활성 사용자 수가 정체되는 현상을 개선하기 위해 서비스 사용 흐름을 분석하는 역할을 맡았습니다. 로그 데이터를 확인한 결과, 가입 이후 첫 핵심 행동에 도달하는 비율이 낮다는 점을 발견했고, 사용자 인터뷰에서도 초기 화면이 복잡해 무엇을 해야 할지 모르겠다는 의견이 반복적으로 나타났습니다. 이를 바탕으로 초기 경험이 단순할수록 핵심 행동 전환율이 높아질 것이라는 가설을 세우고 개선 방향을 정의했습니다. 이 가설을 검증하기 위해 기존 온보딩 흐름과 단순화한 흐름을 동시에 운영하는 실험을 설계했습니다. 핵심 행동 도달률과 D1 잔존율을 주요 지표로 설정하고, 각 흐름의 성과를 비교해 실제 사용자 반응을 확인했습니다. 실험 결과 단순화한 흐름에서 핵심 행동 전환율이 유의미하게 높게 나타나, 이를 기준으로 서비스 전반에 적용하기로 결정했습니다. 이 과정에서 개발·디자인 팀과 함께 IA와 화면 구조를 수정하고, 기능 노출 규칙과 예외 처리를 정책 문서로 정리해 구현 기준을 명확히 했습니다. 또한 실험 결과를 바탕으로 효과가 낮은 기능 요청은 보류하고, 지표 개선에 기여하는 항목에 리소스를 집중해 로드맵을 조정했습니다. 이후에도 지표를 지속적으로 모니터링하며 가설을 업데이트해 서비스 경험을 반복적으로 개선했습니다."

출력:
{"1": 85, "2": 85, "3": 85, "4": 85, "5": 85, "6": 85, "7": 85, "8": 65, "9": 45, "10": 85}

근거:
- KPI 1(85): 로그 분석으로 핵심 행동 도달률이 낮다는 구조적 문제 정의, 사용자 인터뷰로 원인 맥락 확인, "초기 경험 단순화 → 전환율 상승"이라는 명확한 가설 수립 → 교과서적인 상
- KPI 2(85): 로그 데이터 + 사용자 인터뷰를 함께 사용, 핵심 행동 전환율·D1 잔존율을 의사결정 지표로 설정, 두 안을 비교하는 구조적 판단 → 강한 상
- KPI 3(85): 기존 온보딩 흐름과 단순화 흐름을 비교 설계, IA와 화면 구조를 실험 결과 기준으로 수정, 이후 개발·디자인의 기준 구조로 사용 → 강한 상
- KPI 4(85): 기능 노출 규칙과 예외 처리를 정책 문서로 정리, 구현 기준이 되는 문서가 존재 → 강한 상
- KPI 5(85): A/B 테스트 설계, 전환율·잔존율 비교, 결과를 기준으로 전면 적용 결정 → 명확한 상
- KPI 6(85): 실험 결과를 기준으로 효과 낮은 기능 요청 제외, 로드맵 재조정, 리소스를 지표 개선 항목에 집중 → 강한 상
- KPI 7(85): 가설 → 실험 → 적용 → 반복 개선까지 직접 주도, 결정된 방향을 끝까지 실행 → 강한 상
- KPI 8(65): 개발·디자인과 함께 IA·정책 수정, 갈등 구조화·의사결정 번역 수준의 증거는 부족 → 보통 중
- KPI 9(45): AI/LLM 관련 기획, 프롬프트, 기능 언급 없음 → 하
- KPI 10(85): 사용자 인터뷰, 행동 로그, 이를 문제 정의와 개선으로 연결 → 강한 상

### 예시 B (구조·정책 중심 PM)
입력:
"서비스가 성장하면서 기능과 화면이 빠르게 늘어나 사용자와 내부 팀 모두가 구조를 이해하기 어려운 상황이 되었습니다. 이를 해결하기 위해 단순히 새로운 기능을 추가하기보다, 서비스의 목적과 사용자 목표를 기준으로 전체 기능 구조를 정리하는 작업을 맡았습니다. 사용자 유형별로 가장 중요한 행동과 흐름을 정의하고, 이를 바탕으로 IA와 핵심 플로우를 재구성해 서비스의 기본 뼈대를 다시 설계했습니다. 이 구조를 기준으로 기능 접근 방식과 노출 조건, 예외 처리 규칙을 정책 문서로 정리해 개발팀과 공유했습니다. 특히 비슷한 요청이 반복적으로 들어오는 상황에서 어떤 기능을 우선 개발하고 어떤 기능은 보류할지 판단할 수 있도록 기준을 마련해, 이후 의사결정이 일관되게 이루어지도록 했습니다. 이러한 정책은 개발과 디자인 논의에서도 기준점으로 활용되어, 각 팀이 같은 방향을 보고 작업할 수 있게 만들었습니다. 또한 새롭게 정의한 구조를 기반으로 로드맵과 일정 계획을 조정해 핵심 사용자 가치와 직접 연결되지 않는 작업은 다음 단계로 이월하고, 현재 단계에서 가장 중요한 개선 과제에 리소스를 집중했습니다. 그 결과 기능 추가 시 혼선이 줄었고, 서비스 확장 과정에서도 일관된 사용자 경험을 유지할 수 있는 기반이 마련되었습니다."

출력:
{"1": 65, "2": 45, "3": 85, "4": 85, "5": 45, "6": 85, "7": 85, "8": 85, "9": 45, "10": 45}

근거:
- KPI 1(65): "기능과 화면이 늘어나 구조를 이해하기 어려움"이라는 문제 정의, 데이터·지표 기반 원인 분해는 없음, 구조 재정의 필요성 중심의 가설 → 보통 중
- KPI 2(45): 지표, 로그, 인터뷰 언급 없음, 판단 근거가 구조적 필요성과 경험 기반 → 하
- KPI 3(85): 사용자 유형·목표 기준으로 IA 재구성, 핵심 플로우와 구조를 다시 설계, 이후 팀 작업의 기준 구조로 사용 → 교과서적 상
- KPI 4(85): 노출 조건, 접근 방식, 예외 처리 규칙을 정책 문서로 명확히 구조화 → 강한 상
- KPI 5(45): A/B 테스트, 가설 검증, 지표 비교 없음 → 하
- KPI 6(85): 로드맵 재조정, 핵심 사용자 가치 기준으로 스코프 조정, 리소스 집중 → 강한 상
- KPI 7(85): 구조 설계, 정책 문서화, 로드맵 재조정까지 직접 주도 → 강한 상
- KPI 8(85): 정책과 구조를 공통 기준으로 제공, 각 팀이 같은 기준으로 논의·작업 → 강한 상
- KPI 9(45): AI/LLM 관련 기획 언급 없음 → 하
- KPI 10(45): 사용자 인터뷰, 행동 데이터, 인사이트 언급 없음, 구조 중심 판단 → 하

### 예시 C (실행·운영 중심 실무 PM)
입력:
"서비스 런칭을 준비하는 과정에서 일정과 기능 범위를 관리하는 역할을 맡아 프로젝트에 참여했습니다. 여러 팀이 동시에 작업을 진행하는 상황에서, 현재 단계에서 반드시 필요한 기능과 이후로 미뤄도 되는 항목을 구분해 정리하며 개발과 디자인의 우선순위를 맞추는 데 집중했습니다. 이를 통해 팀이 가장 중요한 목표에 집중할 수 있도록 방향을 제시했습니다. 개발과 디자인 진행 상황을 주기적으로 확인하며, 이슈가 발생할 경우 관련 인원과 바로 논의해 해결 방안을 찾았습니다. 기능 구현이 지연되거나 변경이 필요한 경우에는 전체 일정에 영향을 최소화할 수 있도록 범위를 조정했고, 핵심 흐름이 유지되도록 관리했습니다. 일부 기능은 최소한의 형태로 먼저 구현해 출시한 뒤, 운영 상황을 보며 점진적으로 개선하는 방식을 선택했습니다. 서비스 출시 이후에는 사용자 문의와 피드백을 정리해 긴급 수정이 필요한 항목을 빠르게 반영했고, 안정화 이후에는 개선 과제를 도출해 다음 작업으로 이어지도록 했습니다. 이 경험을 통해 PM으로서 실행과 조율을 통해 서비스가 실제로 운영될 수 있도록 만드는 역할의 중요성을 체감하게 되었습니다."

출력:
{"1": 65, "2": 45, "3": 45, "4": 65, "5": 45, "6": 85, "7": 85, "8": 65, "9": 45, "10": 65}

근거:
- KPI 1(65): "일정 지연·기능 누락"이라는 운영 문제 인식, 구조적 원인 분해·가설 트리는 없음 → 보통 중
- KPI 2(45): 지표·로그·인터뷰 근거 없음, 일정·상태 중심 판단 → 하
- KPI 3(45): 화면 구조, 사용자 흐름, IA 설계 언급 없음 → 하
- KPI 4(65): "반드시 필요한 기능 / 이후로 미룰 기능" 정리, 기본적인 요구사항 구조화 → 보통 중
- KPI 5(45): 실험, 비교, 지표 기반 판단 없음 → 하
- KPI 6(85): 핵심 기능 vs 나중 기능 구분, 스코프 축소 후 런칭, 일정 유지 전략 → 강한 상 (이 예시의 핵심 강점)
- KPI 7(85): 이슈 발생 시 즉시 조율, 런칭 → 안정화 → 개선까지 이어짐 → 강한 상
- KPI 8(65): 진행 상황 공유, 이슈 발생 시 관련 인원과 논의, 갈등 구조화까지는 아님 → 보통 중
- KPI 9(45): AI 기획·프롬프트·기능 언급 없음 → 하
- KPI 10(65): 사용자 피드백 수집, 긴급 수정 반영 → 보통 중

### 예시 D (요청 처리형 PM)
입력:
"여러 부서와 이해관계자로부터 전달되는 기능 요청과 개선 사항을 정리하고, 이를 개발팀과 공유하는 역할을 맡아 프로젝트에 참여했습니다. 운영 과정에서 접수되는 사용자 문의와 내부 요청을 모아 우선적으로 반영해야 할 항목을 정리했고, 기능 추가나 수정이 필요할 경우 관련 내용을 문서로 정리해 전달했습니다. 이를 통해 개발팀이 어떤 작업을 진행해야 하는지 명확히 이해할 수 있도록 지원했습니다. 서비스 운영 중 발생하는 이슈나 일정 변경 사항이 있을 때에는 관련 팀에 공유하며 조율 역할을 수행했고, 기능 구현 상태를 확인해 이해관계자에게 진행 상황을 전달했습니다. 사용자 불편 사항이 접수되면 해당 내용을 정리해 개선 요청으로 전환했고, 수정된 기능이 실제로 반영되었는지 확인하는 역할도 맡았습니다. 기획 과정에서는 기존에 정해진 서비스 방향과 요청 사항을 바탕으로 작업을 정리하는 데 집중했으며, 이를 통해 프로젝트가 일정과 범위 내에서 진행될 수 있도록 관리했습니다. 다양한 요청을 빠르게 처리하고 팀 간 소통을 원활하게 하는 경험을 쌓으며, 서비스 운영 전반의 흐름과 PM의 역할을 이해할 수 있었습니다."

출력:
{"1": 45, "2": 45, "3": 45, "4": 65, "5": 45, "6": 65, "7": 65, "8": 65, "9": 45, "10": 65}

근거:
- KPI 1(45): "요청과 문의가 많다"는 현상 인식, 구조적 원인 분석이나 가설 분해 없음, 요청을 문제로 받아들이는 수준 → 하
- KPI 2(45): 지표, 로그, 인터뷰 언급 없음, 요청과 문의를 기반으로 판단 → 하
- KPI 3(45): 화면 흐름, 사용자 구조, IA 설계 없음, 기존 방향을 따름 → 하
- KPI 4(65): 요청·이슈를 문서로 정리해 전달, 기능 수정·추가 요청 구조화, 다만 정책·규칙 수준은 아님 → 보통 중
- KPI 5(45): 실험, 비교, 지표 기반 판단 없음, 요청 → 반영 흐름 → 하
- KPI 6(65): "우선적으로 반영해야 할 항목" 정리, 일정과 범위 내에서 관리, 효과·리스크 기반 판단은 아님 → 보통 중
- KPI 7(65): 요청 수집 → 전달 → 반영 확인, 담당 영역은 끝까지 수행 → 보통 중
- KPI 8(65): 여러 부서 요청 수합, 개발팀·이해관계자와 소통, 다만 갈등 구조화·의사결정 번역은 없음 → 보통 중
- KPI 9(45): AI, 프롬프트, 자동화 기획 언급 없음 → 하
- KPI 10(65): 사용자 문의·불편 사항 수집, 이를 개선 요청으로 전환, 정성 리서치·인사이트 수준은 아님 → 보통 중

### 예시 E (감각·아이디어 중심 PM)
입력:
"사용자에게 더 흥미롭고 매력적인 경험을 제공하는 것을 목표로 서비스 기획에 참여했습니다. 다양한 기능 아이디어와 화면 구성에 대한 제안을 하며 팀 논의에 적극적으로 참여했고, 경쟁 서비스와 최근 트렌드를 참고해 개선 방향을 제시했습니다. 예를 들어 사용자가 더 자주 서비스를 방문하도록 하기 위해 추천 영역과 시각적 요소를 강화하는 방안을 제안했고, 이를 바탕으로 초기 화면 구성을 수정했습니다. 기능을 출시한 이후에는 사용자들의 반응과 팀 내 피드백을 참고해 추가 개선 방향을 논의했습니다. 어떤 기능이 더 주목받는지, 어떤 화면이 더 직관적인지에 대해 의견을 모아 다음 기획에 반영하려 했으며, 빠르게 아이디어를 적용하고 반응을 확인하는 방식으로 서비스를 발전시키고자 했습니다. 여러 아이디어를 동시에 검토하며 다양한 가능성을 열어두는 것이 중요하다고 판단했고, 이를 통해 서비스의 방향성을 탐색했습니다. 개발 및 디자인 팀과 협업하며 기획 내용을 공유했고, 변경되는 요구사항을 반영해 화면과 기능 구성을 조정했습니다. 이 경험을 통해 사용자 경험을 개선하기 위한 아이디어 제안과 방향성 설정이 PM 역할에서 중요한 부분이라는 것을 체감하게 되었습니다."

출력:
{"1": 45, "2": 45, "3": 45, "4": 45, "5": 45, "6": 45, "7": 65, "8": 65, "9": 45, "10": 65}

근거:
- KPI 1(45): "더 흥미롭고 매력적인 경험"이라는 방향성, 구조적 문제 정의나 원인 가설 없음, 아이디어 제안 중심 → 하
- KPI 2(45): 사용자 반응·팀 피드백 언급은 있으나, 지표, 로그, 인터뷰 분석 없음, 판단 근거가 정성적 감각 → 하
- KPI 3(45): 화면 일부 수정, 전체 구조·핵심 플로우 설계 없음 → 하
- KPI 4(45): 변경 사항을 반영했다는 수준, 정책·규칙·요구사항 문서화 증거 없음 → 하
- KPI 5(45): 빠르게 적용 후 반응을 보는 방식, A/B, 지표 비교, 가설 검증 없음 → 하
- KPI 6(45): 여러 아이디어를 동시에 검토, 스코프·리소스 판단 근거 없음 → 하
- KPI 7(65): 아이디어 제안 → 화면 수정 → 반복 개선, 담당 영역에 대한 실행은 존재 → 보통 중
- KPI 8(65): 개발·디자인과 협업, 변경 사항 공유 및 반영, 갈등 조율·의사결정 구조화는 아님 → 보통 중
- KPI 9(45): AI, 프롬프트, 자동화 기획 언급 없음 → 하
- KPI 10(65): 사용자 반응·피드백 참고, 정성적 수준의 공감 → 보통 중
"""


def evaluate_resume_kpis(resume_text: str) -> Dict[int, int]:
    """
    이력서 텍스트를 LLM이 직접 평가하여 PM KPI별 점수 산출.
    
    Few-shot Learning 방식으로 예시를 제공하고,
    LLM이 동일한 기준으로 새 이력서를 평가.
    
    Args:
        resume_text: 이력서/경력 텍스트
    
    Returns:
        {kpi_id: 점수} (점수는 40~90 범위의 정수)
    """
    client = OpenAI(api_key=settings.OPENAI_API_KEY)
    
    system_prompt = f"""너는 PM(Product Manager) 역량 평가 전문가다.
주어진 이력서/경력 텍스트를 읽고, 10개 KPI에 대해 점수를 매긴다.

{KPI_DEFINITIONS}

{FEW_SHOT_EXAMPLES}

## 출력 형식
반드시 JSON 형식으로만 응답해. 설명 없이 점수만 출력.
{{"1": 점수, "2": 점수, ..., "10": 점수}}

## 점수 부여 규칙 (중요!)
- 상 수준: 75~90 범위에서 근거 강도에 따라 차등 (예: 강한 상=88, 보통 상=82, 약한 상=76)
- 중 수준: 55~70 범위에서 근거 강도에 따라 차등 (예: 강한 중=68, 보통 중=62, 약한 중=56)
- 하 수준: 40~50 범위에서 근거 강도에 따라 차등 (예: 약간 언급=48, 거의 없음=44, 전무=40)

⚠️ 절대 45, 65, 85로 딱 떨어지게 점수를 매기지 마라. 반드시 범위 내에서 세밀하게 차등을 두어라.
"""

    user_prompt = f"""다음 이력서를 평가해줘:

{resume_text}

JSON 형식으로 10개 KPI 점수를 출력해."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,  # 약간의 변동성 허용
            response_format={"type": "json_object"}
        )
        
        result = response.choices[0].message.content
        scores = json.loads(result)
        
        # KPI ID를 int로 변환하고 범위 제한
        return {
            int(kpi_id): max(40, min(90, int(score))) 
            for kpi_id, score in scores.items()
        }
    
    except Exception as e:
        print(f"LLM 평가 오류: {e}")
        # 오류 시 기본값 반환
        return {i: 45 for i in range(1, 11)}
